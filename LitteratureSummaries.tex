\documentclass[12pt]{article}
\usepackage[margin=48pt]{geometry}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{subfigure}
\usepackage[usenames]{color}

\begin{document}

\thispagestyle{empty}
\hrule
\vspace{1em}
\begin{center}
{\Huge Litterature summaries} \\
\end{center}
\vspace{2em}
\hrule
\vspace{4em}

\tableofcontents
\vfill
\hrule
\newpage
\setcounter{page}{1}

\section{Spiking Neurons}

\subsection{Simple model of spiking neurons \cite{izhikevich2003simple}}
\textbf{Abstract} : A model is presented that reproduces spiking and bursting behavior of known types of cortical neurons. The model combines the biologically plausibility of Hodgkin-Huxley-type dynamics and the computational efficiency of integrate-and-fire neurons. Using this model, one can simulate tens of thousands of spiking cortical neurons in real time (1 ms resolution) using a desktop PC.

\textbf{Equation} : 
\begin{equation}
\dot{v} = 0.04 v^2 + 5 v + 140 - u + I
\end{equation}
\begin{equation}
\dot{u} = a(bv-u)
\end{equation}
with the auxiliary after-spike resetting \begin{equation}
\texttt{if } v \geq 30 \textnormal{ mV} \texttt{, then }
 \left\{
 \begin{array}{ll}
        v \leftarrow c \\
        u \leftarrow u + d
 \end{array} 
 \right.
\end{equation}

$v$ represents the membrane potential of the neuron and $u$ represents a membrane recovery variable. $a$ and $b$ describe respectively the time scale and the sensitivity of the recovery variable $u$. $c$ and $d$ describes the after-spike reset value respectively of the membrane potential $v$ and the recovery variable $u$.

Typical values are : $a=0.02 \quad b = 0.2, \quad c = -65 \textnormal{ mV}, \quad d=2$

\section{Spiking Synapses}

\subsection{Spike-timing-dependent plasticity (STDP) \cite{gerstner2014neuronal}}
The synapse is the biological element present in between two connected neurons called presynaptic and postsynaptic neurons. The synapse regulates the amount of current from the presynaptic neuron that is transmitted to the postsynaptic neuron. The presynaptic current can be both reduced or amplified by the synapse and then injected in the postsynaptic neuron. The synaptic plasticity includes the mechanisms that involve changes of the synaptic weight : the factor from presynaptic current to postsynaptic current.
Hebbian learning is a general mathematical formulation of Hebb's postulate based on two particular aspects : the change of the synaptic efficacy depend only on (spatially) local variables and depend on the joint activity of pre- and postsynaptic neurons. Numerous learning rules have been derived from rate-based models of synaptic plasticity. Pair-based models of STDP consider the temporal difference between pre- and postsynaptic spikes for the change in weight of a synapse. If we introduce $S_j = \sum_f \delta(t-t^f_j)$ and $S_i = \sum_f \delta(t-t^f_i)$ for the spike trains of pre- and postsynaptic neurons respectively, the update rule is : 
\begin{equation}
\frac{\textnormal{d} w_{ij}}{\textnormal{d}t} =
- A_- (w_{ij}) e^{- \frac{\mid \Delta t \mid}{\tau_-}} S_j + 
A_+ (w_{ij}) e^{- \frac{\mid \Delta t \mid}{\tau_+}} S_i
\end{equation}
where $A_{\pm}(w)$ describes the dependence of the update on the current weight of the synapse, $\Delta t = \mid t_{post}-t_{pre} \mid$ the temporal difference and $\tau_\pm$ the time constant of the exponential decay.

\subsection{Optimal spike-timing dependent plasticity for precise action potential firing in supervised learning \cite{pfister2006optimal}}

Supervised learning paradigm to derive a synaptic update rule that optimizes by gradient ascent the likelihood of postsynaptic firing at one or several desired firing times.
Define the probability density of an entire spike train. It allows to describe explicitly the likelihood of emitting a set of spikes. Since the likelihood is a smooth function (density dist.) it is differentiable with respect to any variable. Thus learning by gradient ascent.

Strong mathematical proof. Relation to STDP. Only single spike, or limited to few spikes.

\subsection{ReSuMe - new supervised learning method for Spiking Neural Networks \cite{ponulak2005resume}}
Original paper, presenting ReSuMe learning rule (first time) for movement control. Combined with a Liquid State Machine. See figure \ref{ReSuMe1}

\begin{figure}
\center
\includegraphics[scale=.4]{ReSuMe_learning_rule1.png}
\caption{ReSuMe learning rule}
\label{ReSuMe1}
\end{figure}

\subsection{Experimental demonstration of learning properties of a new supervised learning method for the spiking neural networks \cite{kasinski2005experimental}}

Experimental demonstration that ReSuMe has the following properties: \begin{enumerate}
\item it can learn temporal sequences of spikes
\item it can learn model object's I/O properties
\item it is scalable
\item it is computationally simple
\item it is fast converging
\end{enumerate}

See figure \ref{ReSuMe1}

\subsection{Supervised learning in spiking neural networks with ReSuMe: sequence learning, classification, and spike shifting \cite{ponulak2010supervised}}

Robust theory describing ReSuMe (Remote Supervised Method) learning rule.
\textbf{how do neurons learn to reproduce template signals encoded in precisly timed sequence of spikes?}

\textit{Supervised learning is believed to be utilized by the neural motor centres to form the internal representation of the body and the environment} (cerebellum and cerebellar cortex)

\textit{It (ReSuMe) is based on the interaction between two spike-timing dependent plasticity (STDP) processes. It appears that ReSuMe represents a spiking analogy to the classical Widrow-Hoff algorithm proposed for the rate-based neuron model (Widrow and Hoff, 1960; Widrow, 1962)}

\textit{It has been demonstrated that ReSuMe enables effective learning of complex temporal and spatio-temporal firing patterns with an arbitrary high accuracy}

\textit{Due to the ability of the method to operate online and due to its fast convergence the method is suitable for real-life applications. [...] SNN trained with ReSuMe become efficient neurocontrollers for movement generation and control}

Widrow-Hoff learning rule (weight change $\Delta w_{oi}$): \begin{equation}
\Delta w_{oi} = \alpha x_i(y_d-y_o)
\end{equation}
$\alpha$: learning rate, $x_i$: input signal, $y_o$: output signal, $y_d$: teaching signal

ReSuMe learning rule (weight change $\frac{\textnormal{d}}{\textnormal{d}t} w_{oi}(t)$): \begin{equation}
\frac{\textnormal{d}}{\textnormal{d}t} w_{oi}(t) = \left[ S_d(t) - S_o(t) \right] \left[ a_d + \int_0^\infty a_{di}(s) S_i(t-s) \textnormal{d}s \right]
\end{equation}
$S_i(t)$, $S_o(t)$ and $S_d(t)$: presynaptic, postsynaptic and target spike trains. The role of the non-correlative factor (Non-hebbian term) $a_d$ is to adjust the average strength of the synaptic inputs so to impose on a neuron a desired level of activity such that the actual mean firing rate of $S_o(t)$ approaches the mean firing rate of signal $S_d(t)$. Thus the task of setting up the precise timing of spikes, is attributed mainly to the Hebbian term $a_{id}(s)$. The kernel $a_{id}(s)$ define the shape of a learning window: \begin{equation}
a_{di}(s) = A_{di} \exp (-\frac{s}{\tau_{di}})
\end{equation}

\begin{enumerate}
\item Learning: sequence of spikes / with various neuron models / in the presence of noise and uncertainity
\item Classification
\item Spike shifting (by setting different parameters)
\end{enumerate}

(The main advantage of VLSI (Maass and Bishop, 1999) systems over processor-based designs lies in their truly parallel mode of operation which fully utilizes the essential property of parallelism found in spiking neural networks.) Advantages: simplicity, scalability, online training suitability, and fast convergence.

See figure \ref{ReSuMe2}
\begin{figure}
\center
\includegraphics[scale=.35]{ReSuMe_learning_rule2.png}
\caption{ReSuMe learning rule}
\label{ReSuMe2}
\end{figure}

\subsection{Apparatus and methods for gating analog and spiking signals in artificial neural networks \cite{ponulak2015apparatus}}

Universal node (neuron) design implementing a universal learning rule in a mixed (analog and spiking) signal spiking network.

\subsection{Gradient Descent for Spiking Neural Network \cite{huh2018gradient}}
Introduce a differentiable synaptic model, which is based on the presynaptic membrane potential. The synaptic current is thus activated gradually throughout the active zone (margin below the threshold potential), this allows to compute a gradient. Uses a regularizer.
Predictive coding task, auto-encoder task, delayed-memory XOR task.

\subsection{Comparison of supervised learning methods for spike time coding in spiking neural networks \cite{kasinski2006comparison}}
See figure \ref{comparisonSupLearn}
\begin{figure}
\center
\includegraphics[scale=.5]{ComparisonSupervisedLearningMethods.png}
\caption{Comparison of supervised learning methods for SNN}
\label{comparisonSupLearn}
\end{figure}

\subsection{Tempotron-like learning with ReSuMe \cite{florian2008tempotron}}
description of the tempotron as a special case of ReSuMe learning rule. Classification task : categorize spike trains not only by firing or not, but also by firing given spike trains.

\textit{Despite its limitations, the tempotron has been proved quite efficient for spoken digit recognition, outperforming with only 15 spiking neurons complex state-of-the-art HMM word recognition systems (Gutig and Sompolinsky, 2008). This shows that spiking neural networks are quite powerful, and using appropriate learning methods for training them might reveal even more of their potential}

\subsection{Span: Spike pattern association neuron for learning spatio-temporal spike patterns \cite{mohemmed2012span}}
Transform spike trains during the learning phase into analog signals (filtered spike trains), based on Widrow-Hoff rule. Compared with ReSuMe and Chronotron.

\textit{Some pattern recognition tasks such as the recognition of colors, visual patterns, odors and sound qualities can not be easily solved by rate-based neural models. Temporal information encoding can also reduce the number of neurons that are necessary to perform a given task.}

\textit{SNN has proved itself adequate for a number of computation and engineering problems. It is considered as a suitable tool to perform temporal pattern recognition and real-time computation}

Learning rule $\Delta w_i$:
\begin{enumerate}
\item with $\alpha$-kernel: $ \widetilde{s}(t) = \sum\limits_{t^f} e \tau^{-1} (t-t^f) e^{-\frac{(t-t^f)}{\tau}} H(t-t^f) $
\begin{equation}
\Delta w_i = \lambda (\frac{e}{2})^2 \left[ \sum_g \sum_f (|t_i^f-t_d^g| + \tau)e^{-\frac{|t_i^f-t_d^g|}{\tau}} - \sum_h \sum_f (|t_i^f-t_a^h| + \tau)e^{-\frac{|t_i^f-t_a^h|}{\tau}} \right]
\end{equation}

\item with exponential kernel: $\widetilde{s}(t - t^f) = H(t-t^f)e^{-\frac{t-t^f}{\tau}}$
\begin{equation}
\delta w_i = \frac{\tau \lambda}{2} \left[ \sum_g \sum_f e^{-\frac{|t_d^g-t_i^f|}{\tau}} - \sum_h \sum_f e^{-\frac{|t_a^h-t_i^f|}{\tau}} \right]
\end{equation}
\end{enumerate}

Illustration of the learning rule on figure \ref{SPAN1}

\begin{figure}
\center
\includegraphics[scale=.35]{SPAN1.png}
\caption{SPAN learning rule}
\label{SPAN1}
\end{figure}

The paper explores: Output spike sequence learning, learning with noise, memory capacity (number of patterns memorized depending on the number of synapses), classification.

\section{Spiking microcircuits}
\subsection{Real-time computing without stable states: A new framework for neural computation based on perturbations \cite{maass2002real}}

\begin{itemize}
\item[-] alternative to paradigms based on Turing machines or attractor neural networks
\item[-] high-dimensional dynamical system (task-independent)
\item[-] universal analog fading memory and universal computational power
\item[-] readouts neurons can learn to extract in real-time from the current neural circuit information about current and past inputs
\item[-] from observation of neocortex's circuits: extremely complex but surprisingly stereotopic microcircuits that can perform a wide spectrum of tasks
\item[-] multiple readouts can be trained to perform different tasks on the same state trajectory of a reccurent neural circuit $\rightarrow$ parallel real-time computing.
\end{itemize}

The liquid has only one attractor state: the resting state. Perturbed state of the liquid represenets present and past inputs $\rightarrow$ potentially providing the information needed for an analysis of various dynamic aspects of the environment.

\begin{enumerate}
\item \textbf{Separation property (SP)}: amount of separation between the trajectories of internal states of the system that are caused by two different input streams.

\item \textbf{Approximation property (AP)}: resolution and recoding capabilities of the readout mechanisms (capability to distinguish and transform differnet internal states of the liquid into given target outputs)


\end{enumerate}

The liquid state $x^M(t)$ is simply the current output of some operator or filter $L^M$ (called liquid filter or liquid circuit) that maps input function $u(\cdot)$ onto functions $x^M(t)$: \begin{equation}
x^M(t) = (L^M u)(t)
\end{equation}

The second component of an LSM $M$ is a memoryless readout map $f^M$ that transforms, at every time $t$, the current liquid state $x^M(t)$ into the output: \begin{equation}
y(t) = f^M(x^M(t))
\end{equation}


\subsection{Polychronization: Computation with spikes \cite{izhikevich2006polychronization}}

Spiking neural network with STDP synaptic weights evolution (1000 Izhikevich neurons). It describes the different rhythms observed, the balance of excitation and inhibition, the rate to spike-timming conversion.

The \textbf{originality of the paper} is the consideration of a large range of synaptic delays, it enables the emergence of polychronous groups (PG), subsets of interconnected neurons activated by a precise spike timimg sequence.

PGs appear in the the context of gamma frequencies. the number of PGs far exceeds the number of neurons in the network, resulting in a large memory capacity of the system.

The paper provides a way to detect PGs from an anatomical point of view.


\subsection{Supervised learning in spiking neural networks with FORCE training \cite{nicola2016supervised}}

Examples include the FORCE method, a novel technique that harnesses chaos to perform computations. We demonstrate the direct applicability of FORCE training to spiking neurons by training networks to mimic various dynamical systems in addition to reproduce more elaborate tasks such as input classification, storing sequences, reproducing the singing behavior of songbirds, and recalling a scene from a movie.

The synaptic weight matrix is the sum of a set of static weights and a set of dynamically varying weights. The time dependent weights,  , are learned using a supervised learning method called Recursive Least Mean Squares (RLMS) so that the squared error between the target dynamics (i.e. the task, $x(t)$) and the network dynamics ($\hat{x}(t)$) is minimized. For the dynamics under consideration, the Izhikevich model had the greatest accuracy and fastest training times. This is partially due to the fact that the Izhikevich neuron has the spike frequency adaptation variable ($u_i$) which operates on a longer time scale (i.e. 100 ms). We set the parameters such that the average firing rate was low ($< 60$ Hz). To test whether a network of spiking neurons can function as a statistical classifier, we trained networks of Izhikevich neurons to classify inputs into two classes separated by either linear or nonlinear boundaries.

The weight matrix : \begin{equation}
\omega_{ij} = G \omega_{ij}^0 + Q \eta_i \phi_j^T
\end{equation}
$\omega_{ij}^0$ the static weight matric that induces chaos, $G$ chaos factor, $Q$ factor increasing desired recurrent dynamics, $\eta_i$ encoding variables, $\phi_j$ decoding variables that are learned with recursive least mean squares: \begin{equation}
\phi (t) = \phi (t-\Delta t) - e(t) P(t) r(t)
\end{equation}
$e(t) = \hat{x}(t) - x(t)$ the error, $P(t)$ estimate for the inverse of the correlation matrix (learning rule) and $r(t)$ the firing rate.

\subsection{Design of the spiking neuron having learning capabilities based on FPGA circuits \cite{kraft2006design}}
Hardware real-time implementations of Spiking Neuron Networks (SNN) with ReSuMe learning rule.

\section{Speech synthesis}

\subsection{A Neural Model to Predict Parameters for a Generalized Command Response Model of Intonation \cite{schnell2018neural}}

Bastian paper. Presents Generalised Command Response (GCR) model, which is extracted from an intonation contour by a matching pursuit algorithm. Constitued by atoms. 

\textit{GCR also enables analysis of the underlying physiological process (approximate muscle response)}.

\textit{Given that GCR approximate muscle responses to neural spikes, it would make sense to use a Spiking Neural Network to generate these atoms. The generated spikes would be filtered by muscle responses to generate the pitch contour.}

\textit{We generalize  the Spike Pattern Association Neuron (SPAN) algorithm from the litterature to construct a loss from GCR atoms, and show thatit can be backpropagated to allow the system to generate natural prosody.}

\section{Other}
\subsection{Software for brain network simulations: a comparative study \cite{tikidji2017software}}



\bibliographystyle{plain}
\bibliography{biblio}

\end{document}